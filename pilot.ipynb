{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Module Graph of Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import ast\n",
    "import pathlib\n",
    "\n",
    "modules_classes_file = 'dataset/module_imports_and_classes.json'\n",
    "\n",
    "\n",
    "\n",
    "def get_all_imports(module_imports_data):\n",
    "    all_imports = list(set(\n",
    "        module_import for node in module_imports_data.values()\\\n",
    "            for module_imports in node.values() \\\n",
    "            for module_import in module_imports['imports'])\n",
    "    )\n",
    "    return all_imports\n",
    "\n",
    "\n",
    "def extract_file_paths(module_imports_data):\n",
    "    node_file_paths = set()\n",
    "    for file_name, file_contents in module_imports_data.items():\n",
    "        file_name = file_name.replace('/', '.').replace('.py', '')\n",
    "        for node_name, _ in file_contents.items():\n",
    "            node_file_path = f\"{file_name}.{node_name}\"\n",
    "            node_file_paths.add(node_file_path)\n",
    "\n",
    "    node_file_paths = list(node_file_paths)\n",
    "    return node_file_paths\n",
    "\n",
    "\n",
    "def get_imports_file_map(module_imports, node_file_paths):\n",
    "    import_packages_map = defaultdict(list)\n",
    "    for module_import in tqdm(module_imports, desc='Creating import file map'):\n",
    "        for fp in node_file_paths:\n",
    "            if fp.endswith(module_import) and fp.split('.')[-1] == module_import.split('.')[-1]:\n",
    "                import_packages_map[module_import].append(fp)\n",
    "    \n",
    "    return import_packages_map\n",
    "\n",
    "\n",
    "def get_used_aliases(tree, aliases):\n",
    "    used_aliases = set()\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Name):\n",
    "            if node.id in aliases:\n",
    "                import_name = aliases[node.id]\n",
    "                used_aliases.add(import_name)\n",
    "    return {u: [] for u in used_aliases}\n",
    "\n",
    "\n",
    "def create_aliases_dict(tree):\n",
    "    all_aliases = dict()\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.Import):\n",
    "            for alias in node.names:\n",
    "                import_as_name = alias.asname if alias.asname else alias.name\n",
    "                all_aliases[import_as_name] = alias.name\n",
    "\n",
    "        elif isinstance(node, ast.ImportFrom):\n",
    "            module_name = node.module\n",
    "            for alias in node.names:\n",
    "                import_as_name = alias.asname if alias.asname else alias.name\n",
    "                all_aliases[import_as_name] = f\"{module_name}.{alias.name}\"\n",
    "    return all_aliases\n",
    "\n",
    "\n",
    "def parse_file(file_path):\n",
    "    file_contents = open(file_path).read()\n",
    "    tree = ast.parse(file_contents)\n",
    "    all_aliases = create_aliases_dict(tree)\n",
    "    all_nodes_contents = dict()\n",
    "    visited = set()\n",
    "\n",
    "    def get_function_details(function_node):\n",
    "        function_imports = get_used_aliases(function_node, all_aliases)\n",
    "        details = {\n",
    "            'type': 'function',\n",
    "            'imports': function_imports,\n",
    "            'docstring': ast.get_docstring(function_node),\n",
    "            'body': ast.get_source_segment(file_contents, function_node),\n",
    "        }\n",
    "        return details\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, ast.ClassDef) and node not in visited:\n",
    "            class_imports = get_used_aliases(node, all_aliases)\n",
    "            all_nodes_contents[node.name] = {\n",
    "                'type': 'class',\n",
    "                'imports': class_imports,\n",
    "                'docstring': ast.get_docstring(node),\n",
    "                'body': ast.get_source_segment(file_contents, node),\n",
    "                'functions': dict()\n",
    "            }\n",
    "            visited.add(node)\n",
    "            for subnode in ast.walk(node):\n",
    "                if isinstance(subnode, ast.FunctionDef) and subnode not in visited:\n",
    "                    all_nodes_contents[node.name]['functions'][subnode.name] = get_function_details(subnode)\n",
    "                    visited.add(subnode)\n",
    "\n",
    "        elif isinstance(node, ast.FunctionDef) and node not in visited:\n",
    "            all_nodes_contents[node.name] = get_function_details(node)\n",
    "            visited.add(node)\n",
    "\n",
    "    return all_nodes_contents\n",
    "\n",
    "\n",
    "def parse_files_in_dir(dir_path):\n",
    "    pkg_paths = pathlib.Path(dir_path).glob('**/*.py')\n",
    "    modules = [str(p) for p in pkg_paths]\n",
    "    all_module_imports = dict()\n",
    "    for i, module in tqdm(enumerate(modules), total=len(modules), desc='Parsing files'):\n",
    "        module_imports_and_classes = parse_file(module)\n",
    "        all_module_imports[module] = module_imports_and_classes\n",
    "        \n",
    "\n",
    "    return all_module_imports\n",
    "\n",
    "def get_init_module_paths(repository):\n",
    "    pkg_paths = pathlib.Path(repository).glob('**/__init__.py')\n",
    "    modules = [str(p).replace('/__init__', '') for p in pkg_paths]\n",
    "    return modules\n",
    "\n",
    "\n",
    "def get_module_to_file_imports(module_imports_data, init_files):\n",
    "    module_node_imports = get_all_imports(module_imports_data)\n",
    "    all_files = list(module_imports_data.keys())\n",
    "    module_to_file = defaultdict(set)\n",
    "\n",
    "    for module_imports_node in tqdm(module_node_imports, desc='Creating module to file map'):\n",
    "        module_imports_node = module_imports_node.split('.')\n",
    "        node, module_import = module_imports_node[-1], '.'.join(module_imports_node[:-1])\n",
    "        \n",
    "        if module_import == '':\n",
    "            continue\n",
    "        \n",
    "        for file in all_files:\n",
    "            file_name = file.replace('/', '.').replace('.py', '')\n",
    "            file_nodes = module_imports_data[file]\n",
    "            if file_name.endswith(module_import) and node in file_nodes:\n",
    "                module_to_file[module_import].add(file)\n",
    "        \n",
    "        for file in init_files:\n",
    "            file_name = file.replace('/', '.').replace('.py', '')\n",
    "            if file_name.endswith(module_import):\n",
    "                module_to_file[module_import].add(file)\n",
    "        \n",
    "        # if module_import == 'llama_index.experimental.param_tuner':\n",
    "        #     print(module_to_file[module_import])\n",
    "        #     break\n",
    "\n",
    "    module_to_file = {k: list(v) for k, v in module_to_file.items()}\n",
    "    with open('module_to_file.json', 'w') as f:\n",
    "        json.dump(module_to_file, f, indent=4)\n",
    "        \n",
    "    return module_to_file\n",
    "\n",
    "\n",
    "def add_files_to_module_imports(module_imports_data, module_to_file):\n",
    "    for _, file_contents in module_imports_data.items():\n",
    "        for _, node_content in file_contents.items():\n",
    "            imports = node_content['imports']\n",
    "            new_imports = dict()\n",
    "            for module_import_node in imports:\n",
    "                node_module_import = module_import_node.split('.')\n",
    "                _, module_import = node_module_import[-1], '.'.join(node_module_import[:-1])\n",
    "                new_imports[module_import_node] = module_to_file[module_import]\\\n",
    "                      if module_import in module_to_file else []\n",
    "            node_content['imports'] = new_imports\n",
    "\n",
    "\n",
    "def add_class_node_to_graph(graph, full_class_name, class_content):\n",
    "    class_name = full_class_name.split('.')[-1]\n",
    "    if not graph.has_node(full_class_name):\n",
    "        graph.add_node(full_class_name, type='class')\n",
    "        graph.nodes[full_class_name]['docstring'] = class_content['docstring']\n",
    "        graph.nodes[full_class_name]['body'] = class_content['body']\n",
    "        graph.nodes[full_class_name]['name'] = class_name\n",
    "        \n",
    "\n",
    "def add_function_node_to_graph(graph, full_function_name, function_content):\n",
    "    function_name = full_function_name.split('.')[-1]\n",
    "    if not graph.has_node(full_function_name):\n",
    "        graph.add_node(full_function_name, type='function')\n",
    "        graph.nodes[full_function_name]['docstring'] = function_content['docstring']\n",
    "        graph.nodes[full_function_name]['body'] = function_content['body']\n",
    "        graph.nodes[full_function_name]['name'] = function_name\n",
    "\n",
    "\n",
    "def clean_file_name(name):\n",
    "    f_name = name.replace('/', '.').replace('.py', '')\n",
    "    f_name = f_name.replace('.__init__', '') if f_name.endswith('.__init__') else f_name\n",
    "    return f_name\n",
    "\n",
    "\n",
    "def create_graph_module_nodes(nxg, file_name):\n",
    "    fp_units = file_name.split('.')\n",
    "\n",
    "    ### Create module nodes\n",
    "\n",
    "    for i in range(1, len(fp_units)):\n",
    "        u = '.'.join(fp_units[:i])\n",
    "        v = '.'.join(fp_units[:i + 1])\n",
    "\n",
    "        if not nxg.has_node(u):\n",
    "            nxg.add_node(u, name=fp_units[i-1], type='module')\n",
    "        \n",
    "        if not nxg.has_node(v):\n",
    "            nxg.add_node(v, name=fp_units[i], type='module')\n",
    "\n",
    "        if not nxg.has_edge(u, v):\n",
    "            nxg.add_edge(u, v, type='module2module')\n",
    "\n",
    "\n",
    "def create_graph_nodes(nxg, module_imports_data):\n",
    "    for file_name, file_contents in tqdm(module_imports_data.items(), desc='Creating graph nodes'):\n",
    "        f_name = clean_file_name(file_name)\n",
    "        create_graph_module_nodes(nxg, f_name)\n",
    "\n",
    "        ### Create class and function nodes\n",
    "\n",
    "        for node_name, node_content in file_contents.items():\n",
    "            full_node_name = f\"{f_name}.{node_name}\"\n",
    "            if node_content['type'] == 'class':\n",
    "                add_class_node_to_graph(nxg, full_node_name, node_content)\n",
    "                \n",
    "                for func_name, func_content in node_content['functions'].items():\n",
    "                    full_func_name = f\"{full_node_name}.{func_name}\"\n",
    "                    add_function_node_to_graph(nxg, full_func_name, func_content)\n",
    "                    nxg.add_edge(full_node_name, full_func_name, type='class2function')\n",
    "\n",
    "            elif node_content['type'] == 'function':\n",
    "                add_function_node_to_graph(nxg, full_node_name, node_content)\n",
    "            \n",
    "            nxg.add_edge(f_name, full_node_name, type=f'module2{node_content[\"type\"]}')\n",
    "            \n",
    "        \n",
    "\n",
    "def create_graph_edges(nxg, module_imports_data, module_to_file):\n",
    "    for file_name, file_contents in tqdm(module_imports_data.items(), desc='Creating graph edges'):\n",
    "        f_name = clean_file_name(file_name)\n",
    "\n",
    "        for node_name, node_content in file_contents.items():\n",
    "            node_module = f\"{f_name}.{node_name}\"\n",
    "            imports = node_content['imports']\n",
    "\n",
    "            for module_imports_node, _ in imports.items():\n",
    "                module_imports_node = module_imports_node.split('.')\n",
    "                _, module_import = module_imports_node[-1], '.'.join(module_imports_node[:-1])\n",
    "                if module_import == '' or module_import not in module_to_file:\n",
    "                    continue\n",
    "\n",
    "                module_file = module_to_file[module_import]\n",
    "                module_file_names = [f.replace('/', '.').replace('.py', '') for f in module_file]\n",
    "            \n",
    "                for module_file_name in module_file_names:\n",
    "                    assert nxg.has_node(node_module), f\"Node not found: {node_module}, {file_name}\"\n",
    "                    assert nxg.has_node(module_file_name), f\"Module not found: {module_file_name}\"\n",
    "                    nxg.add_edge(node_module, module_file_name, type='module2module')\n",
    "                    \n",
    "                    \n",
    "def create_nxg(module_imports_data, module_to_file):\n",
    "    nxg = nx.DiGraph()\n",
    "    # print('Creating graph nodes')\n",
    "    create_graph_nodes(nxg, module_imports_data)\n",
    "    # print('Creating graph edges')\n",
    "    create_graph_edges(nxg, module_imports_data, module_to_file)\n",
    "    add_parent_attribute(nxg)\n",
    "    return nxg\n",
    "\n",
    "\n",
    "def add_parent_attribute(nxg):\n",
    "    visited = set()\n",
    "    def dfs_util(node):\n",
    "        visited.add(node)\n",
    "        for neighbour in nxg.neighbors(node):\n",
    "            nxg.nodes[neighbour]['parent'] = node\n",
    "            if neighbour not in visited:\n",
    "                dfs_util(neighbour)\n",
    "    \n",
    "    for node in nxg.nodes:\n",
    "        if node not in visited:\n",
    "            dfs_util(node)\n",
    "\n",
    "def create_module_graph(repository, f_name=modules_classes_file):\n",
    "    if os.path.exists(f_name):\n",
    "        with open(f_name, 'r') as f:\n",
    "            module_imports_data = json.load(f)\n",
    "            \n",
    "\n",
    "    module_imports_data = parse_files_in_dir(repository)\n",
    "    init_files = get_init_module_paths(repository)\n",
    "    module_to_file = get_module_to_file_imports(module_imports_data, init_files)\n",
    "    add_files_to_module_imports(module_imports_data, module_to_file)\n",
    "\n",
    "    with open(f_name, 'w') as f:\n",
    "        json.dump(module_imports_data, f, indent=4)\n",
    "    nxg = create_nxg(module_imports_data, module_to_file)\n",
    "    write_nxg(f'{repository}_module_graph.gpickle')\n",
    "    \n",
    "    return nxg\n",
    "\n",
    "def load_nxg(repository):\n",
    "    with open(f'{repository}_module_graph.gpickle', 'rb') as f:\n",
    "        nxg = pickle.load(f)\n",
    "    return nxg\n",
    "\n",
    "def write_nxg(nxg, f_name):\n",
    "    with open(f_name, 'wb') as f:\n",
    "        pickle.dump(nxg, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Module Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository = 'llama_index_local'\n",
    "nxg = create_module_graph(repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxg = load_nxg(repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20604, 32824)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nxg.number_of_nodes(), nxg.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def find_nodes_within_distance(graph, start_node, distance):\n",
    "    q, visited = deque(), dict()\n",
    "    q.append((start_node, 0))\n",
    "    \n",
    "    while q:\n",
    "        n, d = q.popleft()\n",
    "        if d <= distance:\n",
    "            visited[n] = d\n",
    "            neighbours = [neighbor for neighbor in graph.neighbors(n) if neighbor != n and neighbor not in visited]\n",
    "            for neighbour in neighbours:\n",
    "                if neighbour not in visited:\n",
    "                    q.append((neighbour, d + 1))\n",
    "    \n",
    "    sorted_list = sorted(visited.items(), key=lambda x: x[1])\n",
    "    return sorted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_node = 'llama_index_local.llama-index-core.llama_index.core.selectors'\n",
    "distance = 2\n",
    "nodes = find_nodes_within_distance(nxg, test_node, distance)\n",
    "print(f\"Nodes within distance {distance} from {test_node}: {len(nodes)}\")\n",
    "for node in nodes:\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating LLM-Generated Docstrings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import streamlit as st\n",
    "import requests\n",
    "from prompt_templates.prompts import SYSTEM_PROMPT\n",
    "\n",
    "\n",
    "openai_apikey = st.secrets[\"OPENAI_API_KEY\"]\n",
    "hf_api_key = st.secrets['HUGGINGFACEHUB_API_TOKEN']\n",
    "any_scale_api_key = st.secrets['ANY_SCALE_API_TOKEN']\n",
    "\n",
    "\n",
    "def get_llm_response(client, model_name, prompt, system_prompt):\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        model=f\"{model_name}\",\n",
    "        messages=[{\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        response = \"Error while generating summary\"\n",
    "        print(e)\n",
    "        print(prompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_any_scale_response(mode_name, user_prompt, system_prompt):\n",
    "    client = openai.OpenAI(\n",
    "        base_url = \"https://api.endpoints.anyscale.com/v1\",\n",
    "        api_key=f\"{any_scale_api_key}\"\n",
    "    )\n",
    "    response = get_llm_response(client, mode_name, user_prompt, system_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_gpt_response(model_name, user_prompt, system_prompt):\n",
    "    client = openai.OpenAI(api_key=f\"{openai_apikey}\")\n",
    "    response = get_llm_response(client, model_name, user_prompt, system_prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_hf_response(model_name, prompt):\n",
    "    headers = {\"Authorization\": f\"Bearer {hf_api_key}\"}\n",
    "    API_URL = f\"https://api-inference.huggingface.co/models/{model_name}\"\n",
    "    summary = requests.post(API_URL, headers=headers, json=prompt)\n",
    "    return summary.json()\n",
    "\n",
    "\n",
    "class LLM():\n",
    "    def __init__(self, llm_config):\n",
    "        self.model_name = llm_config['model_id']\n",
    "        self.model_type = llm_config['type']\n",
    "\n",
    "    def get_response(self, prompt, system_prompt=SYSTEM_PROMPT):\n",
    "        if self.model_type == 'openai':\n",
    "            summary = get_gpt_response(self.model_name, prompt, system_prompt)\n",
    "        elif self.model_type == 'hf':\n",
    "            summary = get_hf_response(self.model_name, prompt, system_prompt)\n",
    "        elif self.model_type == 'anyscale':\n",
    "            summary = get_any_scale_response(self.model_name, prompt, system_prompt)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from prompt_templates.prompts import (\n",
    "    FUNC_SUMMARIZATION_PROMPT, \n",
    "    COMBINE_FUNCTION_SUMMARIZATION_PROMPT,\n",
    "    CLASS_SUMMARIZATION_PROMPT,\n",
    "    COMBINE_CLASS_SUMMARIZATION_PROMPT,\n",
    "    COMBINE_MODULE_SUMMARIZATION_PROMPT\n",
    ")\n",
    "from langchain_text_splitters import (\n",
    "    Language,\n",
    "    RecursiveCharacterTextSplitter,\n",
    ")\n",
    "\n",
    "TOKEN_LIMIT = 50000\n",
    "CHUNK_SIZE = 20000\n",
    "CHUNK_OVERLAP = 2000\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "summaries_dir = 'summaries'\n",
    "\n",
    "def summarize_docs(llm, document, node_prompt, combine_prompt):\n",
    "    # print(\"Summarizing document:\", document)\n",
    "    docs = python_splitter.create_documents([document])\n",
    "    split_summaries = list()\n",
    "    for doc in docs:\n",
    "        prompt = f\"{node_prompt}\\n\\n{doc.page_content}\"\n",
    "        summary = llm.get_response(prompt)\n",
    "        # summary = f\"Summary of: {prompt}\"\n",
    "        split_summaries.append(summary)\n",
    "    \n",
    "    summaries = \"\\n\".join(split_summaries)\n",
    "    combine_summary_prompt = f\"{combine_prompt}\\n\\n Summaries: \\n\\n{summaries}\"\n",
    "    response = llm.get_response(combine_summary_prompt)\n",
    "    # response = f\"Summary of: {combine_summary_prompt}\"\n",
    "    return response\n",
    "\n",
    "\n",
    "def get_prompt_response(llm, prompt, node_prompt, combine_prompt):\n",
    "    num_tokens = len(prompt.split())\n",
    "    if num_tokens >= TOKEN_LIMIT:\n",
    "        response = summarize_docs(llm, prompt, node_prompt, combine_prompt)\n",
    "    else:\n",
    "        prompt = f\"{node_prompt}\\n\\n{prompt}\"\n",
    "        response = llm.get_response(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "def summarize_code_node(llm, node, nxg):\n",
    "    # print(\"Summarizing code node: \", node)\n",
    "    node_type = nxg.nodes[node]['type']\n",
    "    body = nxg.nodes[node]['body']\n",
    "    node_prompt = FUNC_SUMMARIZATION_PROMPT \\\n",
    "        if node_type == 'function' else CLASS_SUMMARIZATION_PROMPT\n",
    "    combine_prompt = COMBINE_FUNCTION_SUMMARIZATION_PROMPT \\\n",
    "        if node_type == 'function' else COMBINE_CLASS_SUMMARIZATION_PROMPT\n",
    "\n",
    "    summary = get_prompt_response(llm, body, node_prompt, combine_prompt)\n",
    "    nxg.nodes[node]['summary'] = summary\n",
    "    return summary\n",
    "\n",
    "\n",
    "def summarize_module_node(llm, node, nxg):\n",
    "    # print(\"Summarising module: \", node)\n",
    "    node_summaries = list()\n",
    "    for neighbour in nxg.neighbors(node):\n",
    "        summary = summarize_node(llm, neighbour, nxg)\n",
    "        node_summaries.append((neighbour, summary))\n",
    "    \n",
    "    combined_summary = \"\\n\".join([f\"{n}:\\n{s}\" for n, s in node_summaries])\n",
    "    node_prompt = COMBINE_MODULE_SUMMARIZATION_PROMPT\n",
    "    combine_prompt = COMBINE_MODULE_SUMMARIZATION_PROMPT\n",
    "    summary = get_prompt_response(llm, combined_summary, node_prompt, combine_prompt)\n",
    "\n",
    "    nxg.nodes[node]['summary'] = summary\n",
    "    return summary\n",
    "\n",
    "\n",
    "def summarize_node(llm, node, nxg):\n",
    "\n",
    "    os.makedirs(summaries_dir, exist_ok=True)\n",
    "    if os.path.exists(f\"{summaries_dir}/{node}.txt\"):\n",
    "        with open(f\"{summaries_dir}/{node}.txt\", 'r') as f:\n",
    "            return f.read()\n",
    "\n",
    "    # print(\"Summarising node: \", node)\n",
    "\n",
    "    node_type = nxg.nodes[node]['type']\n",
    "    summary = summarize_module_node(llm, node, nxg) if node_type == 'module' else summarize_code_node(llm, node, nxg)\n",
    "\n",
    "    with open(f\"{summaries_dir}/{node}.txt\", 'w') as f:\n",
    "        f.write(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "llms = json.load(open('llms.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "nxg = load_nxg(repository)\n",
    "\n",
    "llm = LLM(llms['mistral8x7b'])\n",
    "for node in tqdm(nxg.nodes, total=nxg.number_of_nodes()):\n",
    "    summarize_node(llm, node, nxg)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \\\n",
    "\"\"\"\n",
    "def load_document(uploaded_files: List[UploadedFile]) -> List[Document]:\n",
    "    # Read documents\n",
    "    temp_dir = tempfile.TemporaryDirectory()\n",
    "    for file in uploaded_files:\n",
    "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
    "        with open(temp_filepath, \"wb\") as f:\n",
    "            f.write(file.getvalue())\n",
    "\n",
    "    reader = SimpleDirectoryReader(input_dir=temp_dir.name)\n",
    "    return reader.load_data()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CMAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
